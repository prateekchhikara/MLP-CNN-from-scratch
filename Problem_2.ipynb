{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Incorporating CNNs\n",
    "\n",
    "* Learning Objective: In this problem, you will learn how to deeply understand how Convolutional Neural Networks work by implementing one.\n",
    "* Provided Code: We provide the skeletons of classes you need to complete. Forward checking and gradient checkings are provided for verifying your implementation as well.\n",
    "* TODOs: you will implement a Convolutional Layer and a MaxPooling Layer to improve on your classification results in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from lib.mlp.fully_conn import *\n",
    "from lib.mlp.layer_utils import *\n",
    "from lib.mlp.train import *\n",
    "from lib.cnn.layer_utils import *\n",
    "from lib.cnn.cnn_models import *\n",
    "from lib.datasets import *\n",
    "from lib.grad_check import *\n",
    "from lib.optim import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (CIFAR-100 with 20 superclasses)\n",
    "\n",
    "In this homework, we will be classifying images from the CIFAR-100 dataset into the 20 superclasses. More information about the CIFAR-100 dataset and the 20 superclasses can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Download the CIFAR-100 data files [here](https://drive.google.com/drive/folders/1imXxTnpkMbWEe41pkAGNt_JMTXECDSaW?usp=share_link), and save the `.mat` files to the `data/cifar100` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: data_train Shape: (40000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_train Shape: (40000,), <class 'numpy.ndarray'>\n",
      "Name: data_val Shape: (10000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_val Shape: (10000,), <class 'numpy.ndarray'>\n",
      "Name: data_test Shape: (10000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_test Shape: (10000,), <class 'numpy.ndarray'>\n",
      "label_names: ['aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables', 'household_electrical_devices', 'household_furniture', 'insects', 'large_carnivores', 'large_man-made_outdoor_things', 'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2']\n",
      "Name: mean_image Shape: (1, 1, 1, 3), <class 'numpy.ndarray'>\n",
      "Name: std_image Shape: (1, 1, 1, 3), <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = CIFAR100_data('data/cifar100/')\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print (\"Name: {} Shape: {}, {}\".format(k, v.shape, type(v)))\n",
    "    else:\n",
    "        print(\"{}: {}\".format(k, v))\n",
    "label_names = data['label_names']\n",
    "mean_image = data['mean_image'][0]\n",
    "std_image = data['std_image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: large_omnivores_and_herbivores\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "image_data = data['data_train'][idx]\n",
    "image_data = ((image_data*std_image + mean_image) * 255).astype(np.int32)\n",
    "plt.imshow(image_data)\n",
    "label = label_names[data['labels_train'][idx]]\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "We will use convolutional neural networks to try to improve on the results from Problem 1. Convolutional layers make the assumption that local pixels are more important for prediction than far-away pixels. This allows us to form networks that are robust to small changes in positioning in images.\n",
    "\n",
    "### Convolutional Layer Output size calculation [2pts]\n",
    "\n",
    "As you have learned, two important parameters of a convolutional layer are its stride and padding. To warm up, we will need to calculate the output size of a convolutional layer given its stride and padding. To do this, open the `lib/cnn/layer_utils.py` file and fill out the TODO section in the `get_output_size` function in the ConvLayer2D class. \n",
    "\n",
    "Implement your function so that it returns the correct size as indicated by the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received (32, 16, 16, 16) and expected [32, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "input_image = np.zeros([32, 28, 28, 3]) # a stack of 32 28 by 28 rgb images\n",
    "\n",
    "in_channels = input_image.shape[-1] #must agree with the last dimension of the input image\n",
    "k_size = 4 \n",
    "n_filt = 16\n",
    "\n",
    "conv_layer = ConvLayer2D(in_channels, k_size, n_filt, stride=2, padding=3)\n",
    "output_size = conv_layer.get_output_size(input_image.shape) \n",
    "\n",
    "print(\"Received {} and expected [32, 16, 16, 16]\".format(output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer Forward Pass [5pts]\n",
    "\n",
    "Now, we will implement the forward pass of a convolutional layer. Fill in the TODO block in the `forward` function of the ConvLayer2D class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received output shape: (1, 4, 4, 2), Expected output shape: (1, 4, 4, 2)\n",
      "Difference:  5.110565335399418e-08\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the convolutional forward function\n",
    "input_image = np.linspace(-0.1, 0.4, num=1*8*8*1).reshape([1, 8, 8, 1]) # a single 8 by 8 grayscale image\n",
    "in_channels, k_size, n_filt = 1, 5, 2\n",
    "\n",
    "weight_size = k_size*k_size*in_channels*n_filt\n",
    "bias_size = n_filt\n",
    "\n",
    "\n",
    "\n",
    "single_conv = ConvLayer2D(in_channels, k_size, n_filt, stride=1, padding=0, name=\"conv_test\")\n",
    "\n",
    "w = np.linspace(-0.2, 0.2, num=weight_size).reshape(k_size, k_size, in_channels, n_filt)\n",
    "b = np.linspace(-0.3, 0.3, num=bias_size)\n",
    "\n",
    "single_conv.params[single_conv.w_name] = w\n",
    "single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "out = single_conv.forward(input_image)\n",
    "\n",
    "print(\"Received output shape: {}, Expected output shape: (1, 4, 4, 2)\".format(out.shape))\n",
    "\n",
    "correct_out = np.array([[\n",
    "   [[-0.03874312, 0.57000324],\n",
    "   [-0.03955296, 0.57081309],\n",
    "   [-0.04036281, 0.57162293],\n",
    "   [-0.04117266, 0.57243278]],\n",
    "\n",
    "  [[-0.0452219, 0.57648202],\n",
    "   [-0.04603175, 0.57729187],\n",
    "   [-0.04684159, 0.57810172],\n",
    "   [-0.04765144, 0.57891156]],\n",
    "\n",
    "  [[-0.05170068, 0.5829608 ],\n",
    "   [-0.05251053, 0.58377065],\n",
    "   [-0.05332038, 0.5845805 ],\n",
    "   [-0.05413022, 0.58539035]],\n",
    "\n",
    "  [[-0.05817946, 0.58943959],\n",
    "   [-0.05898931, 0.59024943],\n",
    "   [-0.05979916, 0.59105928],\n",
    "   [-0.06060901, 0.59186913]]]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Layer Backward [5pts]\n",
    "\n",
    "Now complete the backward pass of a convolutional layer. Fill in the TODO block in the `backward` function of the ConvLayer2D class. Check you results with this code and expect differences of less than 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimg Error:  1.8963855769063798e-07\n",
      "dw Error:  1.2923846241327273e-08\n",
      "db Error:  2.706223458995419e-10\n",
      "dimg Shape:  (15, 8, 8, 3) (15, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the conv backward function\n",
    "img = np.random.randn(15, 8, 8, 3)\n",
    "w = np.random.randn(4, 4, 3, 12)\n",
    "b = np.random.randn(12)\n",
    "dout = np.random.randn(15, 4, 4, 12)\n",
    "\n",
    "single_conv = ConvLayer2D(input_channels=3, kernel_size=4, number_filters=12, stride=2, padding=1, name=\"conv_test\")\n",
    "single_conv.params[single_conv.w_name] = w\n",
    "single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "dimg_num = eval_numerical_gradient_array(lambda x: single_conv.forward(img), img, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: single_conv.forward(img), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: single_conv.forward(img), b, dout)\n",
    "\n",
    "out = single_conv.forward(img)\n",
    "\n",
    "dimg = single_conv.backward(dout)\n",
    "dw = single_conv.grads[single_conv.w_name]\n",
    "db = single_conv.grads[single_conv.b_name]\n",
    "\n",
    "# The error should be around 1e-6\n",
    "print(\"dimg Error: \", rel_error(dimg_num, dimg))\n",
    "# The errors should be around 1e-8\n",
    "print(\"dw Error: \", rel_error(dw_num, dw))\n",
    "print(\"db Error: \", rel_error(db_num, db))\n",
    "# The shapes should be same\n",
    "print(\"dimg Shape: \", dimg.shape, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 8, 8, 3), (15, 8, 8, 3))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimg_num.shape, dimg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling Layer\n",
    "Now we will implement maxpooling layers, which can help to reduce the image size while preserving the overall structure of the image.\n",
    "\n",
    "### Forward Pass max pooling [5pts]\n",
    "Fill out the TODO block in the `forward` function of the MaxPoolingLayer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received output shape: (1, 3, 3, 1), Expected output shape: (1, 3, 3, 1)\n",
      "Difference:  1.8750000280978013e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the convolutional forward function\n",
    "input_image = np.linspace(-0.1, 0.4, num=64).reshape([1, 8, 8, 1]) # a single 8 by 8 grayscale image\n",
    "\n",
    "maxpool= MaxPoolingLayer(pool_size=4, stride=2, name=\"maxpool_test\")\n",
    "out = maxpool.forward(input_image)\n",
    "\n",
    "print(\"Received output shape: {}, Expected output shape: (1, 3, 3, 1)\".format(out.shape))\n",
    "\n",
    "correct_out = np.array([[\n",
    "   [[0.11428571],\n",
    "   [0.13015873],\n",
    "   [0.14603175]],\n",
    "\n",
    "  [[0.24126984],\n",
    "   [0.25714286],\n",
    "   [0.27301587]],\n",
    "\n",
    "  [[0.36825397],\n",
    "   [0.38412698],\n",
    "   [0.4       ]]]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass Max pooling [5pts]\n",
    "Fill out the `backward` function in the MaxPoolingLayer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimg Error:  3.282445232628976e-12\n",
      "dimg Shape:  (15, 8, 8, 3) (15, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "img = np.random.randn(15, 8, 8, 3)\n",
    "\n",
    "dout = np.random.randn(15, 3, 3, 3)\n",
    "\n",
    "maxpool= MaxPoolingLayer(pool_size=4, stride=2, name=\"maxpool_test\")\n",
    "\n",
    "dimg_num = eval_numerical_gradient_array(lambda x: maxpool.forward(img), img, dout)\n",
    "\n",
    "out = maxpool.forward(img)\n",
    "dimg = maxpool.backward(dout)\n",
    "\n",
    "# The error should be around 1e-8\n",
    "print(\"dimg Error: \", rel_error(dimg_num, dimg))\n",
    "# The shapes should be same\n",
    "print(\"dimg Shape: \", dimg.shape, img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Small Convolutional Neural Network [3pts]\n",
    "Please find the `TestCNN` class in `lib/cnn/cnn_models.py`.\n",
    "Again you only need to complete few lines of code in the TODO block.\n",
    "Please design a Convolutional --> Maxpool --> flatten --> fc network where the shapes of parameters match the given shapes.\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively.\n",
    "Here you only modify the param_name part, the _w, and _b are automatically assigned during network setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Passed!\n",
      "Testing test-time forward pass ... \n",
      "5.3041079084437115e-08\n",
      "Passed!\n",
      "Testing the loss ...\n",
      "Passed!\n",
      "Testing the gradients (error should be no larger than 1e-6) ...\n",
      "conv_b relative error: 3.90e-09\n",
      "conv_w relative error: 9.74e-10\n",
      "fc1_b relative error: 8.77e-11\n",
      "fc1_w relative error: 3.83e-07\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = TestCNN()\n",
    "loss_func = cross_entropy()\n",
    "\n",
    "B, H, W, iC = 4, 8, 8, 3 #batch, height, width, in_channels\n",
    "k = 3 #kernel size\n",
    "oC, Hi, O = 3, 27, 5 # out channels, Hidden Layer input, Output size\n",
    "std = 0.02\n",
    "x = np.random.randn(B,H,W,iC)\n",
    "y = np.random.randint(O, size=B)\n",
    "\n",
    "print (\"Testing initialization ... \")\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "w1_std = abs(model.net.get_params(\"conv_w\").std() - std)\n",
    "b1 = model.net.get_params(\"conv_b\").std()\n",
    "w2_std = abs(model.net.get_params(\"fc1_w\").std() - std)\n",
    "b2 = model.net.get_params(\"fc1_b\").std()\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "assert w1_std < std / 10, \"First layer weights do not seem right\"\n",
    "assert np.all(b1 == 0), \"First layer biases do not seem right\"\n",
    "assert w2_std < std / 10, \"Second layer weights do not seem right\"\n",
    "assert np.all(b2 == 0), \"Second layer biases do not seem right\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing test-time forward pass ... \")\n",
    "w1 = np.linspace(-0.7, 0.3, num=k*k*iC*oC).reshape(k,k,iC,oC)\n",
    "w2 = np.linspace(-0.2, 0.2, num=Hi*O).reshape(Hi, O)\n",
    "b1 = np.linspace(-0.6, 0.2, num=oC)\n",
    "b2 = np.linspace(-0.9, 0.1, num=O)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "model.net.assign(\"conv_w\", w1)\n",
    "model.net.assign(\"conv_b\", b1)\n",
    "model.net.assign(\"fc1_w\", w2)\n",
    "model.net.assign(\"fc1_b\", b2)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "feats = np.linspace(-5.5, 4.5, num=B*H*W*iC).reshape(B,H,W,iC)\n",
    "scores = model.forward(feats)\n",
    "correct_scores = np.asarray([[-13.85107294, -11.52845818,  -9.20584342,  -6.88322866,  -4.5606139 ],\n",
    " [-11.44514171, -10.21200524 , -8.97886878 , -7.74573231 , -6.51259584],\n",
    " [ -9.03921048,  -8.89555231 , -8.75189413 , -8.60823596,  -8.46457778],\n",
    " [ -6.63327925 , -7.57909937 , -8.52491949 , -9.4707396 , -10.41655972]])\n",
    "scores_diff = np.sum(np.abs(scores - correct_scores))\n",
    "print(scores_diff)\n",
    "assert scores_diff < 1e-6, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the loss ...\",)\n",
    "y = np.asarray([0, 2, 1, 4])\n",
    "loss = loss_func.forward(scores, y)\n",
    "dLoss = loss_func.backward()\n",
    "correct_loss = 4.56046848799693\n",
    "assert abs(loss - correct_loss) < 1e-10, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the gradients (error should be no larger than 1e-6) ...\")\n",
    "din = model.backward(dLoss)\n",
    "for layer in model.net.layers:\n",
    "    if not layer.params:\n",
    "        continue\n",
    "    for name in sorted(layer.grads):\n",
    "        f = lambda _: loss_func.forward(model.forward(feats), y)\n",
    "        grad_num = eval_numerical_gradient(f, layer.params[name], verbose=False)\n",
    "        print ('%s relative error: %.2e' % (name, rel_error(grad_num, layer.grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network [25pts]\n",
    "In this section, we defined a `SmallConvolutionalNetwork` class for you to fill in the TODO block in `lib/cnn/cnn_models.py`.\n",
    "\n",
    "Here please design a network with at most two convolutions and two maxpooling layers (you may use less).\n",
    "You can adjust the parameters for any layer, and include layers other than those listed above that you have implemented (such as fully-connected layers and non-linearities).\n",
    "You are also free to select any optimizer you have implemented (with any learning rate).\n",
    "\n",
    "You will train your network on CIFAR-100 20-way superclass classification.\n",
    "Try to find a combination that is able to achieve 40% validation accuracy.\n",
    "\n",
    "Since the CNN takes significantly longer to train than the fully connected network, it is suggested to start off with fewer filters in your Conv layers and fewer intermediate fully-connected layers so as to get faster initial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange the data\n",
    "data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"], data[\"labels_train\"]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (40000, 32, 32, 3)\n",
      "Flattened data input size: 3072\n",
      "Number of data classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", data_dict[\"data_train\"][0].shape)\n",
    "print(\"Flattened data input size:\", np.prod(data[\"data_train\"].shape[1:]))\n",
    "print(\"Number of data classes:\", max(data['labels_train']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4000 [00:03<3:39:28,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 4000) Average loss: 2.991099808520179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [3:21:04<00:00,  3.02s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h3/grnq7qd92d75g_ctv392hmq40000gn/T/ipykernel_23031/2395206837.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m results = train_net(data_dict, model, loss_f, optimizer, batch_size, epochs, \n\u001b[0;32m---> 27\u001b[0;31m                     lr_decay, lr_decay_every, show_every=4000, verbose=True, regularization=regularization, reg_lambda=reg_lambda)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mopt_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/USC/DL/Assignment-1/csci566-assignment1/lib/mlp/train.py\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(data, model, loss_func, optimizer, batch_size, max_epochs, lr_decay, lr_decay_every, show_every, verbose, regularization, reg_lambda)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# respectively                                                              #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;31m# valloader = DataLoader(data_val, labels_val, batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/USC/DL/Assignment-1/csci566-assignment1/lib/mlp/train.py\u001b[0m in \u001b[0;36mcompute_acc\u001b[0;34m(model, data, labels, num_samples, batch_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/USC/DL/Assignment-1/csci566-assignment1/lib/mlp/fully_conn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feat, is_training, seed)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/USC/DL/Assignment-1/csci566-assignment1/lib/cnn/layer_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;31m# Compute the dot product with the filter weights and add the bias term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                         \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreceptive_field\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m#                             END OF YOUR CODE                              #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = SmallConvolutionalNetwork()\n",
    "loss_f = cross_entropy()\n",
    "\n",
    "\n",
    "results = None\n",
    "#############################################################################\n",
    "# TODO: Use the train_net function you completed to train a network         #\n",
    "# You may only adjust the hyperparameters within this block                 #\n",
    "#############################################################################\n",
    "optimizer = Adam(model.net, 1e-3)\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 1\n",
    "lr_decay = .999\n",
    "lr_decay_every = 10\n",
    "regularization = \"none\"\n",
    "reg_lambda = 0.01\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "results = train_net(data_dict, model, loss_f, optimizer, batch_size, epochs, \n",
    "                    lr_decay, lr_decay_every, show_every=4000, verbose=True, regularization=regularization, reg_lambda=reg_lambda)\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to generate the training plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100]  # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.plot(val_acc_hist, '-o', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Layers [5pts]\n",
    "\n",
    "An interesting finding from early research in convolutional networks was that the learned convolutions resembled filters used for things like edge detection. Complete the code below to visualize the filters in the first convolutional layer of your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_array = None\n",
    "nrows, ncols = None, None\n",
    "\n",
    "###################################################\n",
    "# TODO: read the weights in the convolutional     #\n",
    "# layer and reshape them to a grid of images to   #\n",
    "# view with matplotlib.                           #\n",
    "###################################################\n",
    "\n",
    "# get the filters from the first convolutional layer\n",
    "filters = model.net.get_params(\"conv1_w\")\n",
    "\n",
    "# normalize the filters to be between 0 and 1\n",
    "filters -= filters.min()\n",
    "filters /= filters.max()\n",
    "\n",
    "# reshape the filters to a grid of images\n",
    "num_filters, _, filter_height, filter_width = filters.shape\n",
    "im_array = filters.reshape(num_filters, filter_height, filter_width).transpose(1, 2, 0)\n",
    "\n",
    "# define the number of rows and columns for the plot\n",
    "nrows = int(np.sqrt(num_filters))\n",
    "ncols = int(np.ceil(num_filters / nrows))\n",
    "\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "plt.imshow(im_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline Question: Comment below on what kinds of filters you see. Include your response in your submission [5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Credit: Analysis on Trained Model [5pts]\n",
    "\n",
    "For extra credit, you can perform some additional analysis of your trained model. Some suggested analyses are:\n",
    "1. Plot the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) of your model's predictions on the test set. Look for trends to see which classes are frequently misclassified as other classes (e.g. are the two vehicle superclasses frequently confused with each other?).\n",
    "2. Implement [BatchNorm](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739) and analyze how the models train with and without BatchNorm.\n",
    "3. Introduce some small noise in the labels, and investigate how that affects training and validation accuracy.\n",
    "\n",
    "You are free to choose any analysis question of interest to you. We will not be providing any starter code for the extra credit. Include your extra-credit analysis as the final section of your report pdf, titled \"Extra Credit\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Please prepare a PDF document `problem_2_solution.pdf` in the root directory of this repository with all plots and inline answers of your solution. Concretely, the document should contain the following items in strict order:\n",
    "1. Training loss / accuracy curves for CNN training\n",
    "2. Visualization of convolutional filters\n",
    "3. Answers to inline questions about convolutional filters\n",
    "\n",
    "Note that you still need to submit the jupyter notebook with all generated solutions. We will randomly pick submissions and check that the plots in the PDF and in the notebook are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f95095c8a0a3097784e12049736c2d4ba0ed5c3ad8d4c368b4ca0a8dce635d82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
